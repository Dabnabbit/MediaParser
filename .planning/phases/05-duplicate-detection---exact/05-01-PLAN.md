---
phase: 05-duplicate-detection---exact
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/routes/jobs.py
  - app/lib/duplicates.py
autonomous: true

must_haves:
  truths:
    - "API returns quality metrics for each file in duplicate groups"
    - "API recommends best file based on resolution then file size"
    - "API returns all data needed for side-by-side comparison"
  artifacts:
    - path: "app/lib/duplicates.py"
      provides: "Quality scoring and recommendation logic"
      exports: ["recommend_best_duplicate", "get_quality_metrics"]
    - path: "app/routes/jobs.py"
      provides: "Enhanced /api/jobs/:id/duplicates endpoint"
      contains: "width.*height.*resolution"
  key_links:
    - from: "app/routes/jobs.py"
      to: "app/lib/duplicates.py"
      via: "import recommend_best_duplicate"
      pattern: "from app.lib.duplicates import"
    - from: "app/lib/duplicates.py"
      to: "app/lib/metadata.py"
      via: "get_image_dimensions call"
      pattern: "get_image_dimensions"
---

<objective>
Enhance the /api/jobs/:id/duplicates endpoint to return quality metrics (resolution, file size, format) for each file in duplicate groups, plus a recommendation for which file to keep.

Purpose: Enable informed duplicate resolution by providing quality comparison data
Output: Enhanced API endpoint returning complete comparison data for duplicate groups
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-duplicate-detection---exact/05-RESEARCH.md

@app/routes/jobs.py
@app/lib/metadata.py
@app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create duplicates library with quality scoring</name>
  <files>app/lib/duplicates.py</files>
  <action>
Create app/lib/duplicates.py with two functions:

1. `get_quality_metrics(file: File) -> dict`:
   - Call get_image_dimensions(file.storage_path) from app.lib.metadata
   - Return dict with: width, height, resolution_mp (width*height/1_000_000), file_size_bytes, format (from mime_type)
   - Handle None values gracefully (return None for missing metrics)

2. `recommend_best_duplicate(files: list[dict]) -> int`:
   - Takes list of file dicts with quality metrics already populated
   - Score each file: resolution * 1_000_000 + file_size_bytes
   - Return file_id with highest score
   - If all resolutions are None, fall back to largest file_size_bytes
   - If all sizes are None, return first file_id

Include type hints and docstrings. Use pathlib.Path for paths.
  </action>
  <verify>
python -c "from app.lib.duplicates import recommend_best_duplicate, get_quality_metrics; print('imports ok')"
  </verify>
  <done>
Library module exists with both functions importable and documented
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance duplicates API endpoint</name>
  <files>app/routes/jobs.py</files>
  <action>
Modify the get_job_duplicates() function in app/routes/jobs.py:

1. Import the new functions:
   `from app.lib.duplicates import recommend_best_duplicate, get_quality_metrics`

2. When building each file dict within a duplicate group, call get_quality_metrics() and merge the result into the file dict:
   ```python
   for f in group_files:
       file_dict = {
           'id': f.id,
           'filename': f.original_filename,
           # ... existing fields ...
       }
       # Call get_quality_metrics and merge into file_dict
       metrics = get_quality_metrics(f)
       file_dict.update(metrics)  # Adds width, height, resolution_mp, file_size_bytes, format
       files.append(file_dict)
   ```

3. After building the complete files list for a group, call recommend_best_duplicate() to get the recommended_id:
   ```python
   recommended_id = recommend_best_duplicate(files)
   ```

4. Update the response format for each group:
   ```python
   {
       'hash': hash_key,
       'match_type': 'exact',
       'file_count': len(files),
       'files': files,  # Now includes quality metrics
       'recommended_id': recommended_id  # Result of recommend_best_duplicate(files)
   }
   ```

5. Add group-level aggregate stats:
   - total_size_bytes: sum of all file sizes in group
   - best_resolution_mp: highest resolution in group

Keep the existing filtering logic (skip files without hash or discarded files).
  </action>
  <verify>
curl -s http://localhost:5000/api/jobs/1/duplicates | python -c "import sys,json; d=json.load(sys.stdin); g=d.get('duplicate_groups',[]); print(f'Groups: {len(g)}'); print('Has quality metrics:', bool(g and 'width' in g[0].get('files',[[]])[0] if g else False))"
  </verify>
  <done>
API returns duplicate groups with width, height, resolution_mp, format for each file, plus recommended_id per group
  </done>
</task>

</tasks>

<verification>
1. Flask server running: `curl http://localhost:5000/api/jobs/1/duplicates` returns valid JSON
2. Response includes `recommended_id` for each duplicate group
3. Each file object has `width`, `height`, `resolution_mp`, `format` fields
4. Recommended file has highest resolution (or largest size if resolution unavailable)
5. Groups still properly filter out discarded files
</verification>

<success_criteria>
- GET /api/jobs/:id/duplicates returns quality metrics for all files in groups
- recommended_id correctly identifies highest-quality file in each group
- No regressions in existing duplicate detection logic
- Library functions are reusable for future perceptual duplicate detection (Phase 6)
</success_criteria>

<output>
After completion, create `.planning/phases/05-duplicate-detection---exact/05-01-SUMMARY.md`
</output>
