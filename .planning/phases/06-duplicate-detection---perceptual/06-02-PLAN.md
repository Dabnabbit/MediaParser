---
phase: 06-duplicate-detection---perceptual
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - app/lib/perceptual.py
  - app/tasks.py
autonomous: true

must_haves:
  truths:
    - "Hamming distance correctly calculates bit differences between two perceptual hash hex strings"
    - "Timestamp clustering groups files within 5-second windows in O(n log n) time"
    - "Within-cluster perceptual analysis correctly identifies duplicates (distance 0-5) and similar (distance 6-20)"
    - "Sequence type detection labels burst (<2s), panorama (2-30s), and similar (>30s) groups"
    - "Detection pipeline runs automatically after file processing completes"
    - "Files without timestamps or perceptual hashes are gracefully skipped"
  artifacts:
    - path: "app/lib/perceptual.py"
      provides: "Complete perceptual duplicate detection library"
      exports: ["hamming_distance", "cluster_by_timestamp", "analyze_cluster", "detect_perceptual_duplicates"]
      min_lines: 120
    - path: "app/tasks.py"
      provides: "Updated task pipeline calling perceptual detection after SHA256 grouping"
      contains: "detect_perceptual_duplicates"
  key_links:
    - from: "app/lib/perceptual.py"
      to: "app/tasks.py"
      via: "detect_perceptual_duplicates() called in process_import_job"
      pattern: "detect_perceptual_duplicates"
    - from: "app/lib/perceptual.py"
      to: "app/models.py"
      via: "Sets similar_group_id, similar_group_confidence, similar_group_type on File objects"
      pattern: "similar_group_id"
---

<objective>
Implement the perceptual duplicate detection algorithm: Hamming distance calculation, timestamp-constrained clustering, within-cluster perceptual analysis, and sequence type detection. Wire it into the post-processing pipeline so detection runs automatically after file import.

Purpose: This is the core algorithmic work of Phase 6 - identifying near-duplicate photos (burst shots, format conversions, panorama fragments) using perceptual hash comparison constrained by timestamp proximity.

Output: Working detection library and pipeline integration that populates similar_group_id, similar_group_confidence, and similar_group_type fields on File records.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/designs/duplicate-detection-system.md
@.planning/phases/06-duplicate-detection---perceptual/06-RESEARCH.md
@.planning/phases/06-duplicate-detection---perceptual/06-01-SUMMARY.md
@app/models.py
@app/lib/hashing.py
@app/tasks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Perceptual Detection Library</name>
  <files>app/lib/perceptual.py</files>
  <action>
    Create `app/lib/perceptual.py` with the following functions, closely following the design document pseudocode:

    **1. `hamming_distance(hash1: str, hash2: str) -> int`**
    - Convert two hex hash strings to integers
    - XOR them and use `int.bit_count()` (Python 3.10+, hardware-accelerated)
    - Return number of differing bits
    - Handle None/empty inputs by returning a high value (e.g., 999)

    **2. `cluster_by_timestamp(files, threshold_seconds=5) -> list[list]`**
    - Accept list of File objects (or dicts with detected_timestamp)
    - Filter to files WITH detected_timestamp (skip None)
    - Sort by detected_timestamp (O(n log n))
    - Scan linearly, grouping files where gap <= threshold_seconds
    - Return list of clusters (each cluster is list of file objects)
    - Only include clusters with 2+ files

    **3. `detect_sequence_type(file_a, file_b) -> str`**
    - Calculate timestamp gap in seconds
    - Return 'burst' if gap < 2 seconds
    - Return 'panorama' if gap < 30 seconds
    - Return 'similar' otherwise
    - Return 'similar' if either file lacks a detected_timestamp

    **4. `distance_to_exact_confidence(distance: int) -> str`**
    - All exact duplicates (0-5) return 'high' (timestamp clustering provides corroboration)

    **5. `distance_to_similar_confidence(distance: int) -> str`**
    - Distance 6-10: return 'high'
    - Distance 11-15: return 'medium'
    - Distance 16-20: return 'low'

    **6. `_generate_group_id() -> str`**
    - Generate a unique group ID using `uuid.uuid4().hex[:16]` (short but unique enough)

    **7. `_merge_into_exact_group(file_a, file_b, distance: int)`**
    - If either file already has exact_group_id, use that; else generate new
    - Set exact_group_id and exact_group_confidence on both files
    - Use distance_to_exact_confidence for confidence

    **8. `_merge_into_similar_group(file_a, file_b, distance: int)`**
    - If either file already has similar_group_id, use that; else generate new
    - Set similar_group_id, similar_group_confidence, similar_group_type on both files
    - Use detect_sequence_type for type
    - Use distance_to_similar_confidence for confidence

    **9. `analyze_cluster(cluster: list)`**
    - For each pair (i, j where j > i) in the cluster:
      - Skip if either file lacks file_hash_perceptual
      - Calculate Hamming distance
      - If distance <= 5: merge into exact group (perceptual duplicate)
      - If 6 <= distance <= 20: merge into similar group
      - If distance > 20: skip (unrelated, coincidental timing)

    **10. `detect_perceptual_duplicates(files, threshold_seconds=5)`**
    - Main entry point called from tasks.py
    - Pass 1: SHA256 exact matches already handled by _mark_duplicate_groups()
    - Pass 2: cluster_by_timestamp(files, threshold_seconds)
    - Pass 3: For each cluster, analyze_cluster(cluster)
    - Log summary: "Detected X exact perceptual groups, Y similar groups across Z clusters"

    **Constants at module level:**
    ```python
    EXACT_THRESHOLD = 5        # Hamming distance 0-5 = exact duplicate
    SIMILAR_THRESHOLD = 20     # Hamming distance 6-20 = similar
    CLUSTER_WINDOW_SECONDS = 5 # Timestamp clustering window
    BURST_THRESHOLD = 2        # Seconds gap for burst detection
    PANORAMA_THRESHOLD = 30    # Seconds gap for panorama detection
    ```

    Include proper docstrings, logging via `logging.getLogger(__name__)`, and type hints.
  </action>
  <verify>
    - `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.lib.perceptual import hamming_distance, cluster_by_timestamp, detect_perceptual_duplicates; print('Import OK')"` succeeds
    - `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.lib.perceptual import hamming_distance; assert hamming_distance('0000000000000000', '0000000000000001') == 1; assert hamming_distance('0000000000000000', '0000000000000000') == 0; assert hamming_distance('ffffffffffffffff', '0000000000000000') == 64; print('Hamming distance tests pass')"` succeeds
  </verify>
  <done>Perceptual detection library exists with all functions from design doc. Hamming distance, clustering, and analysis functions work correctly.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate Detection into Task Pipeline</name>
  <files>app/tasks.py</files>
  <action>
    In `app/tasks.py`, add perceptual detection after the existing SHA256 duplicate marking:

    1. Import at top of file: `from app.lib.perceptual import detect_perceptual_duplicates`

    2. In `process_import_job()`, after the call to `_mark_duplicate_groups(db, job)` (around line 361), add:
    ```python
    # Detect perceptual duplicates (near-matches via dHash comparison)
    detect_perceptual_duplicates(job.files)
    ```

    3. The function modifies File objects in-place (setting similar_group_id etc.), and these changes will be committed with the existing `db.session.commit()` that follows.

    4. Also update `_mark_duplicate_groups()` to set `exact_group_confidence = 'high'` on all SHA256-matched files (since SHA256 is byte-identical, confidence is always high):
    ```python
    for file in files:
        file.exact_group_id = hash_value
        file.exact_group_confidence = 'high'
    ```

    NOTE: The detection runs AFTER all files are processed, not during per-file processing. This is correct because we need all perceptual hashes available before comparing.
  </action>
  <verify>
    - `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.tasks import process_import_job; print('Task import OK')"` succeeds
    - `grep -n "detect_perceptual_duplicates" /home/dab/Projects/MediaParser/app/tasks.py` shows the function call in the pipeline
    - `grep -n "exact_group_confidence" /home/dab/Projects/MediaParser/app/tasks.py` shows confidence being set in _mark_duplicate_groups
  </verify>
  <done>Perceptual detection runs automatically after file processing. SHA256 groups have confidence set. Pipeline is complete end-to-end.</done>
</task>

</tasks>

<verification>
1. `from app.lib.perceptual import hamming_distance, cluster_by_timestamp, analyze_cluster, detect_perceptual_duplicates` imports cleanly
2. Hamming distance returns correct values for known inputs
3. Pipeline in tasks.py calls detect_perceptual_duplicates after SHA256 grouping
4. SHA256 duplicate groups have exact_group_confidence = 'high'
</verification>

<success_criteria>
- Perceptual detection library implements all functions from design doc
- Hamming distance uses hardware-accelerated int.bit_count()
- Timestamp clustering achieves O(n log n) complexity
- Detection pipeline runs automatically when job completes
- Confidence mapping matches design doc thresholds
</success_criteria>

<output>
After completion, create `.planning/phases/06-duplicate-detection---perceptual/06-02-SUMMARY.md`
</output>
