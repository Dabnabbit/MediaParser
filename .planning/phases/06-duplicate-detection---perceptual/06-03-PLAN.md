---
phase: 06-duplicate-detection---perceptual
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - app/routes/jobs.py
  - app/routes/review.py
autonomous: true

must_haves:
  truths:
    - "GET /api/jobs/:id/duplicates returns groups using exact_group_id with match_type field distinguishing 'sha256' from 'perceptual'"
    - "GET /api/jobs/:id/similar-groups returns similar groups with type (burst/panorama/similar), confidence, and quality metrics"
    - "GET /api/jobs/:id/summary returns separate counts for exact_duplicate_groups and similar_groups"
    - "GET /api/jobs/:id/files supports mode='similar' to filter files with similar_group_id"
    - "POST /api/similar-groups/:group_id/resolve accepts keep_file_ids array (multiple files kept)"
    - "Discard and undiscard operations handle similar_group_id correctly"
  artifacts:
    - path: "app/routes/jobs.py"
      provides: "Updated endpoints for two-tier duplicate detection"
      contains: "similar_group_id"
    - path: "app/routes/review.py"
      provides: "Similar group resolution and updated discard logic"
      contains: "similar_group_id"
  key_links:
    - from: "app/routes/jobs.py"
      to: "app/models.py"
      via: "File.similar_group_id and File.exact_group_id queries"
      pattern: "similar_group_id"
    - from: "app/routes/review.py"
      to: "app/models.py"
      via: "File.similar_group_id updates in resolve/discard"
      pattern: "similar_group_id"
---

<objective>
Update all API endpoints to support two-tier duplicate detection: exact duplicates (SHA256 + perceptual distance 0-5) and similar groups (perceptual distance 6-20, burst/panorama/similar). Add new endpoints for similar group queries and resolution. Update summary counts and mode filtering.

Purpose: The API layer bridges the detection algorithm (Plan 02) with the UI (Plan 04). After this plan, the backend fully supports querying and resolving both exact and similar duplicate groups.

Output: All API endpoints updated for two-tier detection. New similar-groups endpoint. Updated summary with both group counts. Mode filtering supports 'similar' mode.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/designs/duplicate-detection-system.md
@.planning/phases/06-duplicate-detection---perceptual/06-01-SUMMARY.md
@app/models.py
@app/routes/jobs.py
@app/routes/review.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update Duplicate Groups Endpoint + Add Similar Groups Endpoint</name>
  <files>app/routes/jobs.py</files>
  <action>
    **Part A: Update GET /api/jobs/:id/duplicates**

    The existing endpoint groups files by SHA256 hash. Update it to ALSO include perceptual-only exact groups (files with matching exact_group_id but different SHA256):

    1. Keep existing SHA256 grouping logic as-is
    2. After SHA256 groups, query for additional perceptual-only exact groups:
       - Files where `exact_group_id IS NOT NULL` AND `discarded == False`
       - Group by `exact_group_id`
       - Only include groups that aren't already captured by SHA256 grouping
    3. Add `match_type` field to each group in the response:
       - `'sha256'` for byte-identical groups
       - `'perceptual'` for perceptual-only groups (exact_group_id set by detection algorithm, not SHA256)
    4. Add `confidence` field from `exact_group_confidence` (will be 'high' for all exact groups)
    5. Continue using `recommend_best_duplicate()` for quality recommendations

    **Part B: Add GET /api/jobs/:id/similar-groups**

    New endpoint that returns similar/sequence groups:

    ```python
    @jobs_bp.route('/api/jobs/<int:job_id>/similar-groups')
    def get_similar_groups(job_id):
        """Return similar file groups (burst, panorama, perceptual matches)."""
        job = db.session.get(Job, job_id)
        if not job:
            return jsonify({'error': 'Job not found'}), 404

        # Get all non-discarded files with similar_group_id
        files = File.query.filter(
            File.job_id == job_id,
            File.similar_group_id.isnot(None),
            File.discarded == False
        ).all()

        # Group by similar_group_id
        groups = {}
        for f in files:
            gid = f.similar_group_id
            if gid not in groups:
                groups[gid] = {
                    'group_id': gid,
                    'group_type': f.similar_group_type or 'similar',
                    'confidence': f.similar_group_confidence or 'medium',
                    'files': [],
                    'recommended_id': None
                }
            groups[gid]['files'].append(_serialize_file_extended(f))

        # Filter to groups with 2+ files, add recommendations
        result = []
        for gid, group in groups.items():
            if len(group['files']) >= 2:
                group['recommended_id'] = recommend_best_duplicate(
                    [f for f in files if f.similar_group_id == gid]
                )
                result.append(group)

        return jsonify({'similar_groups': result})
    ```

    **Part C: Update GET /api/jobs/:id/summary**

    Add separate counts for exact duplicate groups and similar groups:

    In the summary endpoint, after existing duplicate count:
    ```python
    # Exact duplicate groups (SHA256 + perceptual exact)
    exact_groups = db.session.query(File.exact_group_id).filter(
        File.job_id == job_id,
        File.exact_group_id.isnot(None),
        File.discarded == False
    ).distinct().count()

    # Similar groups
    similar_groups = db.session.query(File.similar_group_id).filter(
        File.job_id == job_id,
        File.similar_group_id.isnot(None),
        File.discarded == False
    ).distinct().count()
    ```

    Add to response: `'similar_groups': similar_groups` (keep existing `'duplicates'` key for backwards compatibility with exact groups)

    **Part D: Update mode filtering in GET /api/jobs/:id/files**

    Add 'similar' mode support:

    ```python
    elif mode == 'similar':
        query = query.filter(
            File.similar_group_id.isnot(None),
            File.discarded == False
        )
    ```

    Update 'unreviewed' mode to also exclude files in similar groups:
    ```python
    elif mode == 'unreviewed':
        query = query.filter(
            File.reviewed_at.is_(None),
            File.discarded == False,
            File.exact_group_id.is_(None),
            File.similar_group_id.is_(None)
        )
    ```

    Add similar count to mode_totals in response:
    ```python
    similar_count = File.query.filter(
        File.job_id == job_id,
        File.similar_group_id.isnot(None),
        File.discarded == False
    ).count()
    # ... add to mode_totals dict as 'similar': similar_count
    ```
  </action>
  <verify>
    - `grep -n "similar-groups" /home/dab/Projects/MediaParser/app/routes/jobs.py` shows new endpoint
    - `grep -n "similar_group_id" /home/dab/Projects/MediaParser/app/routes/jobs.py` shows mode filtering and summary updates
    - `grep -n "match_type" /home/dab/Projects/MediaParser/app/routes/jobs.py` shows match_type in duplicate groups response
  </verify>
  <done>Jobs API supports two-tier duplicate detection with separate endpoints for exact and similar groups, updated summary counts, and 'similar' mode filtering.</done>
</task>

<task type="auto">
  <name>Task 2: Update Resolution and Discard Logic for Similar Groups</name>
  <files>app/routes/review.py</files>
  <action>
    **Part A: Add similar group resolution endpoint**

    ```python
    @review_bp.route('/api/similar-groups/<group_id>/resolve', methods=['POST'])
    def resolve_similar_group(group_id):
        """Resolve a similar group by keeping selected files and discarding the rest.

        Body: { keep_file_ids: [int, ...] }
        Unlike exact duplicates (keep one), similar groups allow keeping multiple files.
        """
        data = request.get_json()
        keep_ids = set(data.get('keep_file_ids', []))

        if not keep_ids:
            return jsonify({'error': 'Must specify at least one file to keep'}), 400

        # Get all files in this similar group
        group_files = File.query.filter(
            File.similar_group_id == group_id,
            File.discarded == False
        ).all()

        if not group_files:
            return jsonify({'error': 'Group not found or already resolved'}), 404

        kept = 0
        discarded = 0
        for f in group_files:
            if f.id in keep_ids:
                # Keep: clear group membership
                f.similar_group_id = None
                f.similar_group_confidence = None
                f.similar_group_type = None
                kept += 1
            else:
                # Discard: mark as discarded and clear group
                f.discarded = True
                f.discarded_at = datetime.utcnow()
                f.similar_group_id = None
                f.similar_group_confidence = None
                f.similar_group_type = None
                discarded += 1

        db.session.commit()
        return jsonify({'kept': kept, 'discarded': discarded})
    ```

    **Part B: Add similar group keep-all endpoint**

    ```python
    @review_bp.route('/api/similar-groups/<group_id>/keep-all', methods=['POST'])
    def keep_all_similar(group_id):
        """Keep all files in a similar group (mark as not similar)."""
        group_files = File.query.filter(
            File.similar_group_id == group_id,
            File.discarded == False
        ).all()

        if not group_files:
            return jsonify({'error': 'Group not found'}), 404

        for f in group_files:
            f.similar_group_id = None
            f.similar_group_confidence = None
            f.similar_group_type = None

        db.session.commit()
        return jsonify({'cleared': len(group_files)})
    ```

    **Part C: Update discard_file() to clear similar_group_id**

    In the existing `discard_file()` function:
    - After clearing `exact_group_id`, also clear:
      ```python
      file.similar_group_id = None
      file.similar_group_confidence = None
      file.similar_group_type = None
      ```

    **Part D: Update bulk_discard() similarly**

    In `bulk_discard()`, for each discarded file also clear similar_group fields.

    **Part E: Update undiscard_file()**

    In `undiscard_file()`, after existing SHA256 re-evaluation for exact_group_id:
    - Similar group re-evaluation is NOT done on undiscard (would require re-running detection pipeline)
    - Document this: similar_group_id stays None after undiscard, user can re-import to re-detect

    **Part F: Update get_file_detail()**

    In `get_file_detail()` response, add:
    ```python
    'similar_group_id': file.similar_group_id,
    'similar_group_confidence': file.similar_group_confidence,
    'similar_group_type': file.similar_group_type,
    'is_similar': file.similar_group_id is not None,
    ```

    **Part G: Add bulk not-similar endpoint**

    ```python
    @review_bp.route('/api/files/bulk/not-similar', methods=['POST'])
    def bulk_not_similar():
        """Remove files from similar groups (mark as not similar)."""
        data = request.get_json()
        file_ids = data.get('file_ids', [])

        files = File.query.filter(File.id.in_(file_ids)).all()
        cleared = 0
        for f in files:
            if f.similar_group_id:
                f.similar_group_id = None
                f.similar_group_confidence = None
                f.similar_group_type = None
                cleared += 1

        db.session.commit()
        return jsonify({'cleared': cleared})
    ```
  </action>
  <verify>
    - `grep -n "similar-groups" /home/dab/Projects/MediaParser/app/routes/review.py` shows resolve and keep-all endpoints
    - `grep -n "not-similar" /home/dab/Projects/MediaParser/app/routes/review.py` shows bulk not-similar endpoint
    - `grep -n "similar_group_id" /home/dab/Projects/MediaParser/app/routes/review.py` shows clearing in discard functions
    - `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app import create_app; app = create_app(); print('App OK')"` succeeds
  </verify>
  <done>Review API supports similar group resolution (keep multiple), keep-all, not-similar operations. Discard/undiscard handle both group types. File details include similar group info.</done>
</task>

</tasks>

<verification>
1. GET /api/jobs/:id/duplicates returns groups with match_type and confidence
2. GET /api/jobs/:id/similar-groups returns groups with type, confidence, and quality metrics
3. GET /api/jobs/:id/summary includes similar_groups count
4. GET /api/jobs/:id/files supports mode='similar' and excludes similar files from 'unreviewed'
5. POST /api/similar-groups/:id/resolve accepts keep_file_ids array
6. Discard operations clear both exact and similar group fields
7. Flask app starts without errors
</verification>

<success_criteria>
- Two-tier API: separate endpoints for exact duplicates and similar groups
- Similar groups return type (burst/panorama/similar) and confidence
- Summary counts updated for both group types
- Mode filtering includes 'similar' mode
- Resolution supports keeping multiple files (checkbox-style)
- Discard/undiscard handle similar_group_id
</success_criteria>

<output>
After completion, create `.planning/phases/06-duplicate-detection---perceptual/06-03-SUMMARY.md`
</output>
