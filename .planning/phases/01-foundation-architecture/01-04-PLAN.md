---
phase: 01-foundation-architecture
plan: 04
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - app/tasks.py
  - huey_config.py
autonomous: true

must_haves:
  truths:
    - "Job can be created in database and enqueued to Huey"
    - "Worker can dequeue job and update status to RUNNING"
    - "Worker updates job status to COMPLETED or FAILED"
    - "Task has Flask application context for database access"
  artifacts:
    - path: "huey_config.py"
      provides: "Huey instance configured for SQLite"
      contains: "SqliteHuey"
    - path: "app/tasks.py"
      provides: "process_import_job task"
      contains: "@huey.task"
  key_links:
    - from: "app/tasks.py"
      to: "app/__init__.py"
      via: "create_app() for context"
      pattern: "create_app"
    - from: "app/tasks.py"
      to: "app/models.py"
      via: "Job model queries"
      pattern: "Job.query"
---

<objective>
Set up Huey task queue with SQLite backend and create job processing task with proper Flask application context handling.

Purpose: Enables background processing that doesn't block the web UI. Workers can process files asynchronously while the web server remains responsive. Addresses INFRA-02 (background job queue) and Success Criteria 2-3 (job creation/enqueuing, worker dequeuing).

Output: Working Huey configuration and a skeleton processing task that demonstrates the job lifecycle.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-architecture/01-RESEARCH.md
@.planning/phases/01-foundation-architecture/01-01-SUMMARY.md
@.planning/phases/01-foundation-architecture/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configure Huey with SQLite backend</name>
  <files>
    huey_config.py
  </files>
  <action>
Create huey_config.py at project root following RESEARCH.md patterns:

```python
"""
Huey task queue configuration.

Uses SQLite backend for simplicity in v1 (single-server deployment).
Can be migrated to Redis for multi-server if needed later.

Run consumer with:
    huey_consumer huey_config.huey -w 2 -k thread
"""
from pathlib import Path
from huey import SqliteHuey

# Database path for Huey queue (separate from app database)
HUEY_DB_PATH = Path(__file__).parent / 'instance' / 'huey_queue.db'

# Ensure instance directory exists
HUEY_DB_PATH.parent.mkdir(parents=True, exist_ok=True)

huey = SqliteHuey(
    name='mediaparser-tasks',
    filename=str(HUEY_DB_PATH),
    immediate=False,  # Don't run tasks synchronously (important for testing)
    utc=True,         # Store all times as UTC
)

# Consumer configuration
# These are used when running: huey_consumer huey_config.huey
CONSUMER_CONFIG = {
    'workers': 2,           # Number of worker threads
    'worker_type': 'thread',  # Use threads (simpler than processes for SQLite)
    'initial_delay': 0.1,   # Initial poll delay
    'backoff': 1.15,        # Backoff multiplier when queue is empty
    'max_delay': 10.0,      # Maximum poll delay
    'periodic': True,       # Enable periodic tasks
    'check_worker_health': True,
    'health_check_interval': 10,
}
```

**Key design decisions:**
- Separate SQLite file for queue (huey_queue.db) vs app data (mediaparser.db)
- Thread-based workers (safer with SQLite than process-based)
- UTC timestamps for queue entries
- immediate=False ensures tasks go to queue even in dev mode
  </action>
  <verify>
    python -c "from huey_config import huey; print(f'Huey configured: {huey.name}')"
  </verify>
  <done>
    - huey_config.py exists with SqliteHuey instance
    - Queue database path uses instance/ directory
    - Consumer configuration documented
  </done>
</task>

<task type="auto">
  <name>Task 2: Create task module with job processing skeleton</name>
  <files>
    app/tasks.py
  </files>
  <action>
Create app/tasks.py with job processing task:

```python
"""
Huey background tasks for media processing.

Tasks run in worker processes/threads, separate from the Flask web server.
Each task that needs database access must create its own Flask application context.
"""
from datetime import datetime, timezone
from typing import Optional
import logging

from huey_config import huey

logger = logging.getLogger(__name__)


def get_app():
    """
    Create Flask application for use in worker context.

    Must be called inside task to avoid import-time side effects.
    """
    from app import create_app
    return create_app()


@huey.task(retries=2, retry_delay=30)
def process_import_job(job_id: int) -> dict:
    """
    Process an import job.

    This is a skeleton that demonstrates the job lifecycle:
    1. Fetch job from database
    2. Update status to RUNNING
    3. Process files (placeholder for now)
    4. Update status to COMPLETED or FAILED

    Args:
        job_id: ID of the Job record to process

    Returns:
        Dictionary with result info
    """
    app = get_app()

    with app.app_context():
        from app import db
        from app.models import Job, JobStatus

        # Fetch job
        job = db.session.get(Job, job_id)
        if job is None:
            logger.error(f"Job {job_id} not found")
            return {'error': f'Job {job_id} not found'}

        # Update to RUNNING
        job.status = JobStatus.RUNNING
        job.started_at = datetime.now(timezone.utc)
        db.session.commit()
        logger.info(f"Job {job_id} started")

        try:
            # TODO: Actual file processing will be implemented in Phase 2
            # For now, just simulate success

            # Update progress (placeholder)
            job.progress_current = job.progress_total
            db.session.commit()

            # Mark completed
            job.status = JobStatus.COMPLETED
            job.completed_at = datetime.now(timezone.utc)
            db.session.commit()
            logger.info(f"Job {job_id} completed")

            return {
                'job_id': job_id,
                'status': 'completed',
                'processed': job.progress_current
            }

        except Exception as e:
            # Mark failed
            job.status = JobStatus.FAILED
            job.error_message = str(e)[:500]  # Truncate long errors
            job.completed_at = datetime.now(timezone.utc)
            db.session.commit()
            logger.error(f"Job {job_id} failed: {e}")

            # Re-raise so Huey handles retry
            raise


@huey.task()
def health_check() -> dict:
    """
    Simple health check task to verify worker is running.

    Can be called from web app to confirm queue is operational.
    """
    return {
        'status': 'ok',
        'timestamp': datetime.now(timezone.utc).isoformat()
    }


def enqueue_import_job(job_id: int) -> str:
    """
    Helper function to enqueue a job from web app.

    Args:
        job_id: ID of the Job record to process

    Returns:
        Huey task ID (can be used to check status)
    """
    result = process_import_job(job_id)
    return result.id  # Huey task ID
```

**Key patterns:**
- get_app() creates Flask app inside task (avoids circular imports)
- with app.app_context() ensures db.session works
- Status transitions: PENDING -> RUNNING -> COMPLETED/FAILED
- Error handling catches exceptions, updates job, then re-raises for Huey retry
- enqueue_import_job() is a helper for web routes to use
  </action>
  <verify>
    python -c "
from huey_config import huey
from app.tasks import process_import_job, health_check, enqueue_import_job
print('Tasks module imports OK')
print(f'Tasks registered: {list(huey._registry.keys())}')
"
  </verify>
  <done>
    - process_import_job task defined with retry logic
    - Task creates Flask app context for database access
    - Status transitions properly handled
    - health_check task for verifying worker operation
    - enqueue_import_job helper function available
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify end-to-end job lifecycle</name>
  <files>
    (no new files - verification script)
  </files>
  <action>
Create a verification script that tests the complete job lifecycle without running the actual worker:

```python
# test_job_lifecycle.py (temporary verification script)
"""
Verify job can be created, enqueued, and processed.

Run this to test the infrastructure before starting the actual worker.
"""
from app import create_app, db
from app.models import Job, JobStatus
from app.tasks import process_import_job

app = create_app()

print("Testing job lifecycle...")

with app.app_context():
    # 1. Create job in database
    job = Job(
        job_type='import',
        status=JobStatus.PENDING,
        progress_total=5
    )
    db.session.add(job)
    db.session.commit()
    job_id = job.id
    print(f"1. Created job {job_id} with status {job.status.value}")

    # 2. Verify job exists
    job = db.session.get(Job, job_id)
    assert job is not None, "Job should exist"
    assert job.status == JobStatus.PENDING, "Job should be pending"
    print(f"2. Verified job exists with status {job.status.value}")

# 3. Process job directly (simulates what worker does)
# Note: We call the task function directly, not via queue
result = process_import_job.call_local(job_id)
print(f"3. Processed job, result: {result}")

with app.app_context():
    # 4. Verify job completed
    job = db.session.get(Job, job_id)
    assert job.status == JobStatus.COMPLETED, f"Job should be completed, got {job.status}"
    assert job.started_at is not None, "Job should have started_at"
    assert job.completed_at is not None, "Job should have completed_at"
    print(f"4. Verified job completed: started={job.started_at}, completed={job.completed_at}")

    # 5. Clean up
    db.session.delete(job)
    db.session.commit()
    print("5. Cleaned up test job")

print("\n=== Job lifecycle test PASSED ===")
```

Run the script to verify:
```bash
python test_job_lifecycle.py
```

After verification succeeds, the script can be deleted or moved to a tests/ directory.
  </action>
  <verify>
    python -c "
from app import create_app, db
from app.models import Job, JobStatus
from app.tasks import process_import_job

app = create_app()

with app.app_context():
    job = Job(job_type='test', status=JobStatus.PENDING, progress_total=1)
    db.session.add(job)
    db.session.commit()
    job_id = job.id

result = process_import_job.call_local(job_id)
print(f'Task result: {result}')

with app.app_context():
    job = db.session.get(Job, job_id)
    print(f'Final status: {job.status.value}')
    assert job.status == JobStatus.COMPLETED
    db.session.delete(job)
    db.session.commit()

print('Job lifecycle OK')
"
  </verify>
  <done>
    - Job created in PENDING status
    - Task processes job and updates to RUNNING then COMPLETED
    - Task handles Flask application context correctly
    - Database operations succeed
  </done>
</task>

</tasks>

<verification>
1. Huey imports: `python -c "from huey_config import huey; print(huey.name)"`
2. Tasks registered: `python -c "from app.tasks import process_import_job; print('OK')"`
3. Job lifecycle test:
   - Create job in PENDING
   - Call task directly (call_local for testing)
   - Verify status becomes COMPLETED
   - Verify started_at and completed_at set
4. Worker startup test (optional, for manual verification):
   ```bash
   huey_consumer huey_config.huey -w 1 -k thread &
   sleep 2
   python -c "from app.tasks import health_check; print(health_check()())"
   # Kill the consumer after test
   ```
</verification>

<success_criteria>
- Huey configured with SQLite backend in instance/huey_queue.db
- process_import_job task handles PENDING -> RUNNING -> COMPLETED/FAILED
- Task properly creates Flask application context for database access
- Job status and timestamps updated correctly
- Retry logic configured (retries=2, retry_delay=30)
- health_check task available for worker verification
- enqueue_import_job helper available for web routes
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-architecture/01-04-SUMMARY.md`
</output>
