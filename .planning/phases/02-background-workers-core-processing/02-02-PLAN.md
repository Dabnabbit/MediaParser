---
phase: 02-background-workers-core-processing
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - app/lib/processing.py
autonomous: true

must_haves:
  truths:
    - "Single file processing extracts metadata, calculates hashes, and scores confidence"
    - "Worker thread returns dict result without database access"
    - "File type mismatch detected via magic bytes"
    - "Non-images return None for perceptual hash (not error)"
  artifacts:
    - path: "app/lib/processing.py"
      provides: "Single file processing pipeline"
      exports: ["process_single_file", "detect_file_type_mismatch"]
  key_links:
    - from: "app/lib/processing.py"
      to: "app/lib/hashing.py"
      via: "import"
      pattern: "from app\\.lib\\.hashing import"
    - from: "app/lib/processing.py"
      to: "app/lib/confidence.py"
      via: "import"
      pattern: "from app\\.lib\\.confidence import"
    - from: "app/lib/processing.py"
      to: "app/lib/metadata.py"
      via: "import"
      pattern: "from app\\.lib\\.metadata import"
---

<objective>
Create single file processing function for use in thread pool workers.

Purpose: Implement the complete processing pipeline for one file that will be called from ThreadPoolExecutor workers. This function must NOT access the database directly (returns dict), enabling safe parallel execution.

Output: app/lib/processing.py with process_single_file function that orchestrates metadata extraction, hashing, and confidence scoring.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/02-background-workers-core-processing/02-CONTEXT.md
@.planning/phases/02-background-workers-core-processing/02-RESEARCH.md
@.planning/phases/02-background-workers-core-processing/02-01-SUMMARY.md

Reference library modules created in 02-01:
@app/lib/hashing.py
@app/lib/confidence.py

Existing library modules:
@app/lib/metadata.py
@app/lib/timestamp.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create single file processing module</name>
  <files>app/lib/processing.py</files>
  <action>
Create app/lib/processing.py with the complete file processing pipeline:

1. `detect_file_type_mismatch(file_path: Path | str) -> tuple[str, str, bool]`
   - Use python-magic to check magic bytes vs file extension
   - Return (extension, detected_type, is_mismatch)
   - Normalize common variations (jpeg -> jpg)
   - Handle missing python-magic gracefully (log warning, return extension-based)

2. `process_single_file(file_path: Path | str, min_year: int = 2000, default_tz: str = 'UTC') -> dict`

   This function runs in ThreadPoolExecutor workers - NO database access!
   Returns a dict with all extracted data for the main thread to commit.

   Pipeline steps:

   a) File validation:
      - Check file exists
      - Get file size via os.path.getsize()
      - Check for type mismatch (log warning if detected)

   b) Calculate hashes:
      - SHA256 via calculate_sha256() - always
      - Perceptual hash via calculate_perceptual_hash() - returns None for non-images

   c) Extract timestamp candidates:
      - EXIF metadata via get_best_datetime() - returns (datetime, source, confidence)
      - Filename parsing via get_datetime_from_name() - returns datetime or None
      - Build list of (datetime, source) tuples

   d) Calculate confidence:
      - Pass candidates to calculate_confidence(candidates, min_year)
      - Returns (selected_dt, confidence_level, all_candidates)

   e) Return result dict:
      ```python
      return {
          'status': 'success',  # or 'error'
          'file_path': str(file_path),
          'file_size_bytes': size,
          'sha256': sha256_hash,
          'perceptual_hash': perceptual_hash,  # May be None
          'detected_timestamp': selected_dt.isoformat() if selected_dt else None,
          'timestamp_source': source,
          'confidence': confidence_level.value,  # String for JSON
          'timestamp_candidates': candidates_json,  # JSON string for storage
          'mime_type': detected_type,
          'error': None  # Set if status='error'
      }
      ```

   f) Error handling:
      - Wrap entire function in try/except
      - On error, return dict with status='error', error=str(exception)
      - Log error with file path and exception
      - Do NOT raise - let main thread decide how to handle

Key requirements from CONTEXT.md:
- Use earliest valid timestamp (selection strategy)
- Apply min_year sanity floor
- Store all candidates for Phase 4 review
- Treat missing timestamps as LOW confidence, not failure

Import all dependencies at module level:
- from pathlib import Path
- from typing import Optional
- import json
- import logging
- import os
- from app.lib.hashing import calculate_sha256, calculate_perceptual_hash
- from app.lib.confidence import calculate_confidence
- from app.lib.metadata import get_best_datetime
- from app.lib.timestamp import get_datetime_from_name

Try to import magic, but handle ImportError gracefully.
  </action>
  <verify>
python3 -c "from app.lib.processing import process_single_file, detect_file_type_mismatch; print('Import OK')"
  </verify>
  <done>
process_single_file function exists, returns dict without database access, handles errors gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add processing module to library exports</name>
  <files>app/lib/__init__.py</files>
  <action>
Update app/lib/__init__.py to export the new processing function:

Add imports:
- from app.lib.processing import process_single_file, detect_file_type_mismatch

The __init__.py should now export:
- From timestamp.py: get_datetime_from_name, convert_str_to_datetime
- From metadata.py: extract_metadata, get_best_datetime, get_file_type, get_image_dimensions
- From hashing.py: calculate_sha256, calculate_perceptual_hash
- From processing.py: process_single_file, detect_file_type_mismatch
- From confidence.py: calculate_confidence, SOURCE_WEIGHTS
  </action>
  <verify>
python3 -c "from app.lib import process_single_file; print('Export OK')"
  </verify>
  <done>
process_single_file exported from app.lib package.
  </done>
</task>

</tasks>

<verification>
1. Processing module imports all dependencies:
   ```bash
   grep -n "from app.lib" app/lib/processing.py
   ```

2. Function returns dict (check return statement):
   ```bash
   grep -n "return {" app/lib/processing.py
   ```

3. No database imports (critical for thread safety):
   ```bash
   grep -n "from app import db" app/lib/processing.py
   # Should return nothing
   ```

4. Error handling present:
   ```bash
   grep -n "except" app/lib/processing.py
   ```

5. Library exports updated:
   ```bash
   grep -n "process_single_file" app/lib/__init__.py
   ```
</verification>

<success_criteria>
1. app/lib/processing.py exists with process_single_file function
2. Function returns dict with all required fields
3. No database access in processing module (thread-safe)
4. Errors caught and returned in dict, not raised
5. All library functions properly imported and used
6. Module exported from app.lib package
</success_criteria>

<output>
After completion, create `.planning/phases/02-background-workers-core-processing/02-02-SUMMARY.md`
</output>
