---
phase: 02-background-workers-core-processing
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - app/tasks.py
autonomous: true

must_haves:
  truths:
    - "Worker dequeues job and processes all files in batch"
    - "File records written to database with metadata, timestamps, and confidence"
    - "Progress updates with current filename and count/total"
    - "Job status checks enable pause/cancel during processing"
    - "Error threshold halts job when failure rate exceeds 10%"
  artifacts:
    - path: "app/tasks.py"
      provides: "Complete import job processing with multi-threading"
      exports: ["process_import_job", "enqueue_import_job"]
  key_links:
    - from: "app/tasks.py"
      to: "app/lib/processing.py"
      via: "import process_single_file"
      pattern: "from app\\.lib\\.processing import"
    - from: "app/tasks.py"
      to: "ThreadPoolExecutor"
      via: "concurrent.futures"
      pattern: "ThreadPoolExecutor"
    - from: "app/tasks.py"
      to: "app/models.py"
      via: "Job and File model updates"
      pattern: "db\\.session\\.(commit|get)"
---

<objective>
Implement complete process_import_job task with multi-threaded file processing.

Purpose: Replace the skeleton process_import_job task with full implementation that processes files using ThreadPoolExecutor, updates File records with extracted metadata, tracks progress, handles errors with threshold, and supports pause/cancel.

Output: Fully functional app/tasks.py that implements Phase 2 file processing requirements.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/02-background-workers-core-processing/02-CONTEXT.md
@.planning/phases/02-background-workers-core-processing/02-RESEARCH.md
@.planning/phases/02-background-workers-core-processing/02-01-SUMMARY.md
@.planning/phases/02-background-workers-core-processing/02-02-SUMMARY.md

Current task skeleton:
@app/tasks.py

Models for reference:
@app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement multi-threaded process_import_job</name>
  <files>app/tasks.py</files>
  <action>
Rewrite process_import_job in app/tasks.py to implement full file processing:

**Imports to add:**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import json
from app.lib.processing import process_single_file
from app.models import Job, File, JobStatus, ConfidenceLevel
```

**Configuration constants:**
```python
BATCH_COMMIT_SIZE = 10  # Commit every N files (Claude discretion from RESEARCH.md)
ERROR_THRESHOLD = 0.10  # Halt job if >10% failures (user decision from CONTEXT.md)
MIN_SAMPLE_SIZE = 10    # Need minimum files before checking threshold
```

**process_import_job(job_id: int) implementation:**

1. **Setup phase:**
   - Get Flask app context via get_app()
   - Fetch job from database
   - Update to RUNNING, set started_at timestamp
   - Get files sorted by original_filename (alphabetical - user decision)
   - Set progress_total = len(files)
   - Get worker thread count from config (default os.cpu_count())
   - Get min_year and default_tz from config

2. **Processing loop with ThreadPoolExecutor:**
   ```python
   max_workers = app.config.get('WORKER_THREADS', os.cpu_count() or 1)
   min_year = app.config.get('MIN_VALID_YEAR', 2000)
   default_tz = app.config.get('TIMEZONE', 'America/New_York')

   processed_count = 0
   error_count = 0
   pending_updates = []  # Batch updates

   with ThreadPoolExecutor(max_workers=max_workers) as executor:
       # Submit all files
       future_to_file = {
           executor.submit(
               process_single_file,
               file.storage_path,
               min_year,
               default_tz
           ): file
           for file in files
       }

       # Process results as they complete
       for future in as_completed(future_to_file):
           file_obj = future_to_file[future]

           # Check for cancellation/pause (every file)
           db.session.refresh(job)
           if job.status in (JobStatus.CANCELLED, JobStatus.PAUSED):
               # Commit pending updates before returning
               _commit_pending_updates(db, pending_updates)
               return {'status': job.status.value, 'processed': processed_count}

           # Get result
           result = future.result()
           processed_count += 1

           # Update progress
           job.progress_current = processed_count
           job.current_filename = file_obj.original_filename

           if result['status'] == 'error':
               error_count += 1
               job.error_count = error_count
               logger.error(f"File error: {result['error']}")

               # Check error threshold
               if _should_halt_job(processed_count, error_count, ERROR_THRESHOLD, MIN_SAMPLE_SIZE):
                   job.status = JobStatus.HALTED
                   job.error_message = f"Error rate {error_count}/{processed_count} exceeded {ERROR_THRESHOLD*100}%"
                   db.session.commit()
                   return {'status': 'halted', 'processed': processed_count, 'errors': error_count}
           else:
               # Queue file update
               pending_updates.append({
                   'file_id': file_obj.id,
                   'result': result
               })

           # Batch commit
           if len(pending_updates) >= BATCH_COMMIT_SIZE:
               _commit_pending_updates(db, pending_updates)
               pending_updates = []
               db.session.commit()  # Also commit job progress
   ```

3. **Finalization:**
   - Commit any remaining pending updates
   - Set job.status = COMPLETED
   - Set job.completed_at = datetime.now(timezone.utc)
   - Clear job.current_filename
   - Return success dict

4. **Error handling:**
   - Wrap in try/except for unexpected errors
   - On exception: set FAILED, store error_message, re-raise for Huey retry

**Helper functions:**

```python
def _should_halt_job(processed: int, errors: int, threshold: float, min_sample: int) -> bool:
    """Check if error rate exceeds threshold."""
    if processed < min_sample:
        return False
    return (errors / processed) > threshold

def _commit_pending_updates(db, pending_updates: list):
    """Apply pending file updates to database."""
    for update in pending_updates:
        file_obj = db.session.get(File, update['file_id'])
        result = update['result']

        file_obj.file_hash_sha256 = result['sha256']
        file_obj.file_hash_perceptual = result['perceptual_hash']
        file_obj.file_size_bytes = result['file_size_bytes']
        file_obj.mime_type = result['mime_type']

        # Parse timestamp if present
        if result['detected_timestamp']:
            from datetime import datetime
            file_obj.detected_timestamp = datetime.fromisoformat(result['detected_timestamp'])

        file_obj.timestamp_source = result['timestamp_source']
        file_obj.confidence = ConfidenceLevel(result['confidence'])
        file_obj.timestamp_candidates = result['timestamp_candidates']

    db.session.flush()  # Flush but don't commit yet
```

Keep existing:
- get_app() function
- health_check() task
- enqueue_import_job() helper

Follow patterns from 01-04:
- get_app() + with app.app_context()
- Status transitions with timestamps
- Error handling with re-raise
  </action>
  <verify>
python3 -c "from app.tasks import process_import_job, _should_halt_job; print('Import OK')"
grep -n "ThreadPoolExecutor" app/tasks.py
grep -n "BATCH_COMMIT_SIZE" app/tasks.py
  </verify>
  <done>
process_import_job implements multi-threaded file processing with progress tracking, error threshold, and pause/cancel support.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add configuration options</name>
  <files>config.py</files>
  <action>
Add Phase 2 configuration options to config.py:

In the base Config class, add:
```python
# Phase 2: Processing Configuration
WORKER_THREADS = None  # None = auto-detect CPU count
MIN_VALID_YEAR = 2000  # Sanity floor for timestamps
BATCH_COMMIT_SIZE = 10  # Files per database commit
ERROR_THRESHOLD = 0.10  # Halt job if error rate exceeds this
```

In DevelopmentConfig (if exists) or ProductionConfig:
- Can override defaults if needed

These allow users to tune processing behavior without code changes.
  </action>
  <verify>
grep -n "WORKER_THREADS" config.py
grep -n "MIN_VALID_YEAR" config.py
  </verify>
  <done>
Processing configuration options available in config.py.
  </done>
</task>

</tasks>

<verification>
1. ThreadPoolExecutor used for parallel processing:
   ```bash
   grep -n "ThreadPoolExecutor" app/tasks.py
   ```

2. Error threshold check implemented:
   ```bash
   grep -n "_should_halt_job\|ERROR_THRESHOLD" app/tasks.py
   ```

3. Batch commits implemented:
   ```bash
   grep -n "BATCH_COMMIT_SIZE\|_commit_pending_updates" app/tasks.py
   ```

4. Pause/cancel check in loop:
   ```bash
   grep -n "CANCELLED\|PAUSED" app/tasks.py
   ```

5. Progress tracking:
   ```bash
   grep -n "progress_current\|current_filename" app/tasks.py
   ```

6. Configuration options exist:
   ```bash
   grep -n "WORKER_THREADS\|MIN_VALID_YEAR" config.py
   ```
</verification>

<success_criteria>
1. process_import_job uses ThreadPoolExecutor for parallel file processing
2. File records updated with SHA256, perceptual hash, timestamp, confidence
3. Progress tracked with current file and count/total
4. Job status checked each file for pause/cancel support
5. Error threshold (10%) halts job with HALTED status
6. Batch commits every 10 files (configurable)
7. Configuration options in config.py for tuning
</success_criteria>

<output>
After completion, create `.planning/phases/02-background-workers-core-processing/02-03-SUMMARY.md`
</output>
