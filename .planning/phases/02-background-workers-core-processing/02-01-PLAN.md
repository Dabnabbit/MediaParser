---
phase: 02-background-workers-core-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/lib/hashing.py
  - app/lib/confidence.py
  - app/models.py
autonomous: true

must_haves:
  truths:
    - "SHA256 hash calculated for any file via streaming (no memory issues)"
    - "Perceptual hash calculated for images using dHash"
    - "Confidence score categorizes timestamps as HIGH/MEDIUM/LOW/NONE"
    - "Job model supports PAUSED, CANCELLED, HALTED statuses"
  artifacts:
    - path: "app/lib/hashing.py"
      provides: "SHA256 and perceptual hash calculation functions"
      exports: ["calculate_sha256", "calculate_perceptual_hash"]
    - path: "app/lib/confidence.py"
      provides: "Confidence scoring with weighted timestamp sources"
      exports: ["calculate_confidence", "SOURCE_WEIGHTS"]
    - path: "app/models.py"
      provides: "Extended JobStatus enum and File.timestamp_candidates field"
      contains: ["PAUSED", "CANCELLED", "HALTED", "timestamp_candidates"]
  key_links:
    - from: "app/lib/hashing.py"
      to: "hashlib.sha256"
      via: "chunked file reading"
      pattern: "sha256\\.update"
    - from: "app/lib/confidence.py"
      to: "ConfidenceLevel enum"
      via: "return value"
      pattern: "ConfidenceLevel\\."
---

<objective>
Create hashing and confidence scoring library modules for file processing.

Purpose: Enable background workers to calculate file hashes (SHA256 for exact duplicates, perceptual for near-duplicates) and determine timestamp confidence scores based on source agreement. These are the core algorithms for Phase 2 processing.

Output: Two new library modules (hashing.py, confidence.py) and extended Job model with additional status values.
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/02-background-workers-core-processing/02-CONTEXT.md
@.planning/phases/02-background-workers-core-processing/02-RESEARCH.md
@.planning/phases/01-foundation-architecture/01-03-SUMMARY.md

Reference existing library patterns:
@app/lib/timestamp.py
@app/lib/metadata.py
@app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create hashing library module</name>
  <files>app/lib/hashing.py</files>
  <action>
Create app/lib/hashing.py with two functions:

1. `calculate_sha256(file_path: Path | str, chunk_size: int = 65536) -> str`
   - Read file in chunks to avoid memory issues with large videos
   - Use hashlib.sha256() with HACL*-backed implementation
   - Return hex digest string (64 chars)
   - Handle Path or str input (convert to Path internally)

2. `calculate_perceptual_hash(file_path: Path | str) -> Optional[str]`
   - Use imagehash.dhash() algorithm (fast, good for duplicates)
   - Wrap in try/except - return None if not an image or file is corrupt
   - Open image with PIL.Image.open()
   - Return hex string representation of hash
   - Log warning for non-images (expected behavior, not error)

Follow existing library patterns:
- Type hints on all parameters and return values
- Docstrings with Args/Returns sections
- Accept Path | str for flexibility (like metadata.py)
- Import logging and create module logger

Add to app/lib/__init__.py exports:
- from app.lib.hashing import calculate_sha256, calculate_perceptual_hash
  </action>
  <verify>
python3 -c "from app.lib.hashing import calculate_sha256, calculate_perceptual_hash; print('Import OK')"
  </verify>
  <done>
calculate_sha256 and calculate_perceptual_hash functions exist with proper type hints and docstrings.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create confidence scoring library module</name>
  <files>app/lib/confidence.py</files>
  <action>
Create app/lib/confidence.py with confidence scoring logic:

1. Define SOURCE_WEIGHTS dict (from RESEARCH.md):
   ```python
   SOURCE_WEIGHTS = {
       'EXIF:DateTimeOriginal': 10,
       'EXIF:CreateDate': 8,
       'QuickTime:CreateDate': 7,
       'EXIF:ModifyDate': 5,
       'filename_datetime': 3,
       'filename_date': 2,
       'File:FileModifyDate': 1,
   }
   ```

2. `calculate_confidence(timestamp_candidates: list[tuple[datetime, str]], min_year: int = 2000) -> tuple[Optional[datetime], ConfidenceLevel, list]`
   - Filter candidates by min_year sanity floor (avoids 1970 epoch dates)
   - Pick earliest valid timestamp (user decision from CONTEXT.md)
   - Check agreement: timestamps within 1 second tolerance = same
   - Score based on source weight and agreement count:
     - HIGH: EXIF source (weight >= 8) AND multiple sources agree
     - MEDIUM: Reliable source (weight >= 5) OR multiple sources agree
     - LOW: Filename only or low-weight source alone
     - NONE: No valid candidates after filtering
   - Return (selected_datetime, confidence_level, all_candidates_for_storage)

Import from app.models import ConfidenceLevel for the enum.

Follow existing patterns from timestamp.py:
- Type hints with Optional where nullable
- Comprehensive docstring with algorithm explanation
- Handle edge cases (empty input, all timestamps before min_year)
  </action>
  <verify>
python3 -c "from app.lib.confidence import calculate_confidence, SOURCE_WEIGHTS; print('Import OK, weights:', len(SOURCE_WEIGHTS))"
  </verify>
  <done>
calculate_confidence function exists with weighted scoring algorithm, SOURCE_WEIGHTS exported.
  </done>
</task>

<task type="auto">
  <name>Task 3: Extend Job model with additional statuses</name>
  <files>app/models.py</files>
  <action>
Extend the existing models for Phase 2 requirements:

1. Add new JobStatus enum values (from CONTEXT.md decisions):
   - PAUSED = "paused" - Job paused by user, can be resumed
   - CANCELLED = "cancelled" - Job cancelled by user, stopped gracefully
   - HALTED = "halted" - Job halted due to error threshold exceeded

2. Add timestamp_candidates field to File model:
   - timestamp_candidates: Mapped[Optional[str]] = mapped_column(Text)
   - Stores JSON array of all detected timestamps with sources
   - Enables Phase 4 review UI to show side-by-side comparison
   - Example: '[{"datetime": "2024-01-15T12:00:00Z", "source": "EXIF:DateTimeOriginal"}, ...]'

3. Add current_filename field to Job model for progress display:
   - current_filename: Mapped[Optional[str]] = mapped_column(String(255))
   - Shows which file is currently being processed

4. Add error_count field to Job model for threshold tracking:
   - error_count: Mapped[int] = mapped_column(Integer, default=0, nullable=False)
   - Tracks number of file processing errors for threshold calculation

Keep all existing fields and relationships intact. Only ADD new items.
  </action>
  <verify>
python3 -c "from app.models import JobStatus, File, Job; print('PAUSED' in [s.value for s in JobStatus]); print(hasattr(File, 'timestamp_candidates'))"
  </verify>
  <done>
JobStatus has PAUSED, CANCELLED, HALTED values; File has timestamp_candidates field; Job has current_filename and error_count fields.
  </done>
</task>

</tasks>

<verification>
1. All imports work without errors:
   ```bash
   python3 -c "from app.lib import calculate_sha256, calculate_perceptual_hash"
   python3 -c "from app.lib.confidence import calculate_confidence"
   python3 -c "from app.models import JobStatus; print([s.value for s in JobStatus])"
   ```

2. Hashing functions have correct signatures:
   ```bash
   grep -n "def calculate_sha256" app/lib/hashing.py
   grep -n "def calculate_perceptual_hash" app/lib/hashing.py
   ```

3. Confidence function uses SOURCE_WEIGHTS:
   ```bash
   grep -n "SOURCE_WEIGHTS" app/lib/confidence.py
   ```

4. New model fields exist:
   ```bash
   grep -n "timestamp_candidates" app/models.py
   grep -n "current_filename" app/models.py
   grep -n "HALTED" app/models.py
   ```
</verification>

<success_criteria>
1. app/lib/hashing.py exists with calculate_sha256 and calculate_perceptual_hash functions
2. app/lib/confidence.py exists with calculate_confidence function and SOURCE_WEIGHTS
3. JobStatus enum includes PAUSED, CANCELLED, HALTED values
4. File model has timestamp_candidates Text field
5. Job model has current_filename and error_count fields
6. All modules import successfully without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-background-workers-core-processing/02-01-SUMMARY.md`
</output>
