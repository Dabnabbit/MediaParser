---
phase: 07-output-generation-tagging
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/tasks.py
  - app/lib/export.py
  - app/routes/jobs.py
autonomous: true

must_haves:
  truths:
    - "Export job copies non-discarded files to year-based output folders"
    - "Output filenames follow YYYYMMDD_HHMMSS.ext format with counter suffix for collisions"
    - "Files without timestamps go to unknown/ subfolder with original filename"
    - "Export job supports pause/cancel/resume and tracks progress"
    - "File.output_path is set after successful copy"
  artifacts:
    - path: "app/lib/export.py"
      provides: "Output filename generation and file copy logic"
      contains: "generate_output_filename"
    - path: "app/tasks.py"
      provides: "process_export_job Huey task"
      contains: "process_export_job"
    - path: "app/routes/jobs.py"
      provides: "Export trigger endpoint"
      contains: "POST /api/jobs/<id>/export"
  key_links:
    - from: "app/routes/jobs.py"
      to: "app/tasks.py"
      via: "enqueue_export_job()"
      pattern: "enqueue_export_job"
    - from: "app/tasks.py"
      to: "app/lib/export.py"
      via: "generate_output_filename() and copy_file_to_output()"
      pattern: "generate_output_filename|copy_file_to_output"
    - from: "app/tasks.py"
      to: "app/models.py"
      via: "File.output_path updated on success"
      pattern: "file_obj\\.output_path"
---

<objective>
Build the export task engine that copies non-discarded files to organized output directories with standardized filenames.

Purpose: This is the core file copy pipeline for Phase 7 - it takes reviewed files and produces the organized output archive with year-based folders and YYYYMMDD_HHMMSS filenames.

Output: `app/lib/export.py` (filename generation + copy logic), `process_export_job()` in `app/tasks.py`, export trigger endpoint in `app/routes/jobs.py`
</objective>

<execution_context>
@/home/dab/.claude/get-shit-done/workflows/execute-plan.md
@/home/dab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@app/tasks.py — Reuse process_import_job pattern (get_app, app_context, batch commits, pause/cancel, error threshold)
@app/models.py — File model (output_path, final_timestamp, detected_timestamp, discarded), Job model (job_type='export')
@app/routes/jobs.py — Existing job management endpoints (add export trigger here)
@config.py — OUTPUT_FOLDER configuration
@app/routes/settings.py — Output directory settings (user may have changed output path)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create export library module with filename generation and file copy</name>
  <files>app/lib/export.py</files>
  <action>
Create `app/lib/export.py` with these functions:

**`generate_output_filename(file_obj, output_base: Path) -> Path`:**
- Determine timestamp: use `file_obj.final_timestamp` first, fall back to `file_obj.detected_timestamp`
- If timestamp exists:
  - Format as `YYYYMMDD_HHMMSS` (e.g., `20240115_120000`)
  - Year subfolder: `output_base / str(year)` (e.g., `output/2024/`)
  - Extension: preserve original extension from `file_obj.original_filename` (lowercase)
  - Full path: `output_base / year / YYYYMMDD_HHMMSS.ext`
- If NO timestamp (both final and detected are None):
  - Place in `output_base / 'unknown'` subfolder
  - Use original filename (cleaned via secure_filename-style sanitization)
- Return the computed output Path

**`resolve_collision(output_path: Path) -> Path`:**
- If `output_path` does not exist, return it as-is
- Otherwise, add counter suffix: `YYYYMMDD_HHMMSS_001.ext`, `_002`, etc.
- Start at `_001` and increment until unique path found
- Max 999 collisions (raise ValueError if exceeded — indicates data issue)
- Return the unique Path

**`copy_file_to_output(source_path: Path, output_path: Path) -> Path`:**
- Create parent directory with `output_path.parent.mkdir(parents=True, exist_ok=True)`
- Resolve collision: `output_path = resolve_collision(output_path)`
- Copy file with `shutil.copy2(source_path, output_path)` to preserve filesystem metadata
- Verify copy: check output file exists and size matches source
- Return the final output path (after collision resolution)

All functions use pathlib.Path. Import shutil for copy2.

For deterministic ordering of same-timestamp files, `generate_output_filename` does NOT handle collisions — that's done by `copy_file_to_output` at copy time. This separation enables the caller to sort files before copying.
  </action>
  <verify>
Run: `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.lib.export import generate_output_filename, resolve_collision, copy_file_to_output; print('Import OK')"`
  </verify>
  <done>Three functions exist in app/lib/export.py: generate_output_filename (timestamp-to-path), resolve_collision (counter suffix), copy_file_to_output (shutil.copy2 with collision resolution). All use pathlib.</done>
</task>

<task type="auto">
  <name>Task 2: Create export Huey task and API endpoint</name>
  <files>app/tasks.py, app/routes/jobs.py</files>
  <action>
**In `app/tasks.py`, add `process_export_job()` and `enqueue_export_job()`:**

Follow the exact same pattern as `process_import_job`:
- `@huey.task(retries=2, retry_delay=30)` decorator
- `get_app()` + `with app.app_context()` for database access
- Fetch job, set status to RUNNING, set started_at

**Export-specific logic:**

1. Get output directory from settings (query Setting model for 'output_directory') or fall back to `app.config['OUTPUT_FOLDER']`
2. Query files to export using windowed approach for memory efficiency:
```python
files_to_export = File.query.join(File.jobs).filter(
    Job.id == job_id,
    File.discarded == False,
    File.output_path.is_(None)  # Not yet exported (resume support)
).order_by(
    File.final_timestamp.asc().nullslast(),
    File.detected_timestamp.asc().nullslast(),
    File.original_filename.asc()
).all()
```
3. Set `job.progress_total = len(files_to_export)`
4. For each file:
   - Generate output path via `generate_output_filename(file_obj, output_base)`
   - Get source path from `file_obj.storage_path or file_obj.original_path`
   - Copy via `copy_file_to_output(source_path, output_path)`
   - Set `file_obj.output_path = str(final_output_path)`
   - Update `job.progress_current` and `job.current_filename`
   - Commit progress every 5 files or first 20 files (same pattern as import)
5. Batch commit pattern: commit every BATCH_COMMIT_SIZE files
6. Pause/cancel check: `db.session.refresh(job)` after each file, check status
7. Error handling: try/except per file, increment error_count, check threshold
8. On completion: set status COMPLETED, completed_at, commit

**Add `enqueue_export_job(job_id)` helper** (same pattern as `enqueue_import_job`):
```python
def enqueue_export_job(job_id: int):
    process_export_job(job_id)
```

**In `app/routes/jobs.py`, add export trigger endpoint:**

`POST /api/jobs/<int:job_id>/export`:
- Verify job exists and is an 'import' type job with status COMPLETED
- Create a new Job record with `job_type='export'`, status PENDING
- Associate the same files as the import job (copy the job_files associations)
- Call `enqueue_export_job(new_job.id)`
- Return `{'job_id': new_job.id, 'status': 'queued', 'file_count': N}`

The export job is a NEW job record linked to the same files. This preserves import job history and enables separate progress tracking.
  </action>
  <verify>
Run: `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.tasks import process_export_job, enqueue_export_job; print('Tasks OK')"` and `cd /home/dab/Projects/MediaParser && .venv/bin/python -c "from app.routes.jobs import jobs_bp; print('Routes OK')"`
  </verify>
  <done>Export Huey task processes non-discarded files with pause/cancel/resume support, batch commits, and error threshold. API endpoint POST /api/jobs/:id/export creates export job and enqueues it. File.output_path set for each successfully copied file.</done>
</task>

</tasks>

<verification>
1. `app/lib/export.py` exists with generate_output_filename, resolve_collision, copy_file_to_output
2. `app/tasks.py` contains process_export_job following same pattern as process_import_job
3. `app/routes/jobs.py` contains POST /api/jobs/:id/export endpoint
4. Export task reads final_timestamp -> detected_timestamp -> unknown/ fallback
5. Year-based subfolder creation works
6. Counter suffix collision handling (_001, _002, etc.)
7. Pause/cancel/resume pattern works (checks File.output_path.is_(None) for resume)
</verification>

<success_criteria>
- Export job copies files to year-based output directories
- Filenames follow YYYYMMDD_HHMMSS.ext format
- Counter suffix handles same-timestamp collisions
- Files without timestamps go to unknown/ subfolder
- Progress tracking, pause/cancel, error threshold all function
- File.output_path populated for each exported file
</success_criteria>

<output>
After completion, create `.planning/phases/07-output-generation-tagging/07-01-SUMMARY.md`
</output>
